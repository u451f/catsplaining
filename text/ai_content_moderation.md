# What is AI content moderation?

## What is an algorithm?

[To do: Harmonize this with what we already wrote on page 68 ]

## What is automation? 

[To do: Harmonize this with what we already wrote on page 72, "Levels of automation".]

First let’s talk about automation and then will get into what content moderation is. At the end we will put them together to give you a good understanding of what goes into automated content moderation, for example, on social media platforms.
So what is automation? There are several terms related to the concept of automation. An algorithm is a simple set of instructions. If you put data in, you get data out.
Now, automation is also a set of instructions but when the algorithm is carried out it leads to a conclusion, decision or an action.
You may have heard of artificial intelligence. This is an umbrella term for a variety of techniques that I will describe next, but essentially AI is automation that aims to approximate human capability.
Techniques to achieve artificial intelligence include machine learning, deep learning and active learning. Machine learning is a form of artificial intelligence algorithm that improves itself based on training data. It “learns from experience.“
There is also deep learning, which is really machine learning that improves itself without training parameters. It “learns how to learn” from data. 
Lastly, active learning is an iteration of deep learning but without predetermined the data set that the algorithm is learning from. 

## What is content moderation?

Let’s talk now about content moderation. Content moderation happens in six phases not necessarily in order. These distinct phases are first definition second detection third evaluation fourth enforcement fifth appeal and six education.
In the first phase of definition, platforms must clearly define what is impermissible content and what is impermissible behaviour such that this is clear and actionable for internal decision-makers but also platform users and finally legal processes. Defining content needing moderation comes in the form of policies, terms of service, community guidelines or rules.
In the detection phase of content moderation platforms can moderate or intercept content before it’s posted or after it’s posted. Platforms can respond to content that has been deemed objectionable by others or a platform can proactively look for content in need of moderation. It needs to both detect content and metadata, which is “data about data”. Metadata, like number, timing or frequency, can closely approximate to behaviour. Platforms need to both look for existing content that is known to be impermissible on the platform as well as look for new and novel content. Lastly in the detection phase, platforms need to look at both multimedia content and language or text.
After detection, platforms move into the evaluation phase of content moderation. They are motivated to evaluate and re-evaluate upon appeal content especially when it involves user reporting, high stakes violations like child sexual abuse material, or high stakes enforcement for an action taken, which is the next phase. A platform ensures content is truly in violation with additional layers of automated detection or manual detection like putting a “human in the loop”.
There are several actions that can result. In the enforcement phase of content moderation content removal is only one of many options. Enforcement could include the introduction of speed bumps, warnings, contextual flags or counter speech. Enforcement might mean filtering for some users who are at work or are too young to view some kinds of content. Enforcement can include freezing the comments section or preventing users from sharing a post. Content that has been moderated might decrease in visibility for other users. It could get demonetised, or remove certain ads from appearing next to it. Platforms also moderate users, through enforcement like account suspension. There is a huge variety of actions that platforms can take to enforce content moderation policies and not all of them are transparent to users or up for appeal.
Speaking of appeal, the next phase in content moderation is very important. The appeal phase exists because errors are inevitable and detection is highly subjective. Although only for some kind of enforcement is appeal possible, platforms are motivated to offer appeals because it improves their automation and gives users recourse especially in cases including user reporting, high stakes violations, or high stakes enforcement. An appeal will often send the content in question back into the evaluation phase.
Lastly we have to mention the importance of education. The education phase in content moderation happens throughout. Notification must be given when users sign up, when an enforcement action is taken, or an appeal is denied because users need to understand, and platforms are obliged to explain their decision-making. The education phase is closely related to the definition phase. Transparency of content moderation policies is a major front of human rights advocacy yet transparency of technical mechanisms detection must be balanced to ensure users are unable to game the system.
Now let’s apply the techniques of automation to the six phases of content moderation. We can see application in at least three phases: detection, evaluation and enforcement.
The use of algorithms to detect objectionable content, both multimedia and text, is hard. For novel content this involves computer vision and natural language processing. For known multimedia content techniques like perceptual hashing, or the fingerprinting of images and videos, can be used.
Improving the accuracy of machine, deep or active learning techniques is resource intensive, which is why the evaluation phase may constitute the application of several layers of automation before involving a human moderator.
And when these artificial intelligence techniques result in a consequence like the many options in the enforcement phase, we are talking about the “automated” aspect of automated content moderation.

## Putting it all together

Talk about bias,
non-english languages,
computer vision coding in english (and other biases),
e2ee environments,
other shortcomings
